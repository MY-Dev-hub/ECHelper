# ECHelper AI ëª¨ë¸ ì—°ë™ ì™„ë²½ ë¶„ì„

ì´ ë¬¸ì„œëŠ” ECHelper í”„ë¡œì íŠ¸ì—ì„œ AI ëª¨ë¸ì´ ì–´ë–»ê²Œ í•™ìŠµë˜ê³ , ì–´ë–»ê²Œ í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œì—ì„œ ì‚¬ìš©ë˜ëŠ”ì§€ë¥¼ **ëª¨ë“  ë³€ìˆ˜ì™€ í•¨ìˆ˜ì˜ ì—°ê²°**, **ìˆ˜í•™ì  ê³„ì‚° ê³¼ì •**ê¹Œì§€ í¬í•¨í•˜ì—¬ ì•„ì£¼ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ğŸ“‹ ëª©ì°¨
1. [ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#1-ì „ì²´-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
2. [AI ëª¨ë¸ í•™ìŠµ ê³¼ì • (train_kobert.py)](#2-ai-ëª¨ë¸-í•™ìŠµ-ê³¼ì •)
3. [ë°±ì—”ë“œ ì˜ˆì¸¡ ì„œë²„ (app.py)](#3-ë°±ì—”ë“œ-ì˜ˆì¸¡-ì„œë²„)
4. [í”„ë¡ íŠ¸ì—”ë“œ í†µì‹  (kobertPrediction.ts)](#4-í”„ë¡ íŠ¸ì—”ë“œ-í†µì‹ )
5. [TF-IDF ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (tfidf.ts)](#5-tf-idf-ìœ ì‚¬-ë¬¸ì„œ-ê²€ìƒ‰)
6. [UI ë Œë”ë§ (PredictionPage.tsx â†’ PredictionResult.tsx)](#6-ui-ë Œë”ë§)
7. [ì „ì²´ ë°ì´í„° íë¦„ ìš”ì•½](#7-ì „ì²´-ë°ì´í„°-íë¦„-ìš”ì•½)

---

## 1. ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ì‚¬ìš©ì ë¸Œë¼ìš°ì €                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ PredictionPage.tsx                                  â”‚     â”‚
â”‚  â”‚   â”œâ”€ PredictionForm (ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°)             â”‚     â”‚
â”‚  â”‚   â”œâ”€ handlePredict() í•¨ìˆ˜ â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚     â”‚
â”‚  â”‚   â”‚                           â”‚                 â”‚   â”‚     â”‚
â”‚  â”‚   â”‚                           â–¼                 â–¼   â”‚     â”‚
â”‚  â”‚   â”‚              kobertPrediction.ts    tfidf.ts    â”‚     â”‚
â”‚  â”‚   â”‚              (ë°±ì—”ë“œ API í˜¸ì¶œ)     (ë¡œì»¬ ê²€ìƒ‰) â”‚     â”‚
â”‚  â”‚   â”‚                    â”‚                      â”‚     â”‚     â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚     â”‚
â”‚  â”‚                         â”‚                            â”‚     â”‚
â”‚  â”‚   PredictionResult â—„â”€â”€â”€â”´â”€ ê²°ê³¼ í‘œì‹œ                 â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP POST /predict
                         â”‚ { text: "..." }
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ë°±ì—”ë“œ ì„œë²„ (Python)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ app.py (FastAPI)                                    â”‚     â”‚
â”‚  â”‚   â”œâ”€ /predict ì—”ë“œí¬ì¸íŠ¸                            â”‚     â”‚
â”‚  â”‚   â”œâ”€ tokenizer (í…ìŠ¤íŠ¸ â†’ ìˆ«ì)                     â”‚     â”‚
â”‚  â”‚   â”œâ”€ model (KoBERT) â”€â”€â”€â”                           â”‚     â”‚
â”‚  â”‚   â”‚                      â”‚                          â”‚     â”‚
â”‚  â”‚   â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚     â”‚
â”‚  â”‚   â”‚      â–¼                                          â”‚     â”‚
â”‚  â”‚   â”‚  [input_ids, attention_mask]                    â”‚     â”‚
â”‚  â”‚   â”‚      â”‚                                          â”‚     â”‚
â”‚  â”‚   â”‚      â–¼                                          â”‚     â”‚
â”‚  â”‚   â”‚  Transformer Layers (12ì¸µ)                      â”‚     â”‚
â”‚  â”‚   â”‚      â”‚                                          â”‚     â”‚
â”‚  â”‚   â”‚      â–¼                                          â”‚     â”‚
â”‚  â”‚   â”‚  Classification Head                            â”‚     â”‚
â”‚  â”‚   â”‚      â”‚                                          â”‚     â”‚
â”‚  â”‚   â”‚      â–¼                                          â”‚     â”‚
â”‚  â”‚   â”‚  logits [batch_size, 2]                         â”‚     â”‚
â”‚  â”‚   â”‚      â”‚                                          â”‚     â”‚
â”‚  â”‚   â”‚      â–¼                                          â”‚     â”‚
â”‚  â”‚   â”‚  Softmax í•¨ìˆ˜                                   â”‚     â”‚
â”‚  â”‚   â”‚      â”‚                                          â”‚     â”‚
â”‚  â”‚   â”‚      â–¼                                          â”‚     â”‚
â”‚  â”‚   â”‚  probs [0.15, 0.85] (í™•ë¥ )                      â”‚     â”‚
â”‚  â”‚   â”‚                                                 â”‚     â”‚
â”‚  â”‚   â””â”€ ê²°ê³¼ ë°˜í™˜: { isStrategic, confidence, ... }   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. AI ëª¨ë¸ í•™ìŠµ ê³¼ì •

### íŒŒì¼: `scripts/train_kobert.py`

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” **KoBERT ëª¨ë¸ì„ ì „ëµë¬¼ì ë¶„ë¥˜ì— ë§ê²Œ Fine-tuning**í•©ë‹ˆë‹¤.

---

### 2.1 ë°ì´í„° ë¡œë“œ (1-33ë²ˆ ì¤„)

```python
# 25ë²ˆ ì¤„
df = pd.read_excel('../data/labelled_data_aug_for_learning.xlsx')

# 31-32ë²ˆ ì¤„: ë°ì´í„° ì¶”ì¶œ
texts = df['data_total'].fillna('').astype(str).tolist()
labels = df['label'].tolist()
```

**ë³€ìˆ˜ ì„¤ëª…:**
- `df`: íŒë‹¤ìŠ¤ DataFrame (ì—‘ì…€ íŒŒì¼ ë‚´ìš©)
  - ì»¬ëŸ¼: `data_total` (í…ìŠ¤íŠ¸ ë°ì´í„°), `label` (0 ë˜ëŠ” 1)
  - 0 = ë¹„ì „ëµë¬¼ì, 1 = ì „ëµë¬¼ì
- `texts`: ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ì›ìë¡œ ë¶€í’ˆ", "ì¼ë°˜ ì»´í“¨í„°", ...])
- `labels`: ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [1, 0, 1, ...])

---

### 2.2 ë°ì´í„° ë¶„í•  (36-40ë²ˆ ì¤„)

```python
train_texts, val_texts, train_labels, val_labels = train_test_split(
    texts, labels, test_size=0.2, random_state=42, stratify=labels
)
```

**ì„¤ëª…:**
- ì „ì²´ ë°ì´í„°ì˜ 80%ë¥¼ í›ˆë ¨ìš©, 20%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í• 
- `stratify=labels`: ê° í´ë˜ìŠ¤(0, 1)ì˜ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©° ë¶„í• 
- `random_state=42`: ì¬í˜„ ê°€ëŠ¥í•œ ëœë¤ ë¶„í• 

**ë³€ìˆ˜:**
- `train_texts`: í›ˆë ¨ìš© í…ìŠ¤íŠ¸ (ì•½ 1,830ê°œ)
- `val_texts`: ê²€ì¦ìš© í…ìŠ¤íŠ¸ (ì•½ 457ê°œ)
- `train_labels`: í›ˆë ¨ìš© ë¼ë²¨
- `val_labels`: ê²€ì¦ìš© ë¼ë²¨

---

### 2.3 KoBERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (44-51ë²ˆ ì¤„)

```python
model_name = "skt/kobert-base-v1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2  # 0: ë¹„ì „ëµë¬¼ì, 1: ì „ëµë¬¼ì
)
```

**KoBERT ëª¨ë¸ êµ¬ì¡°:**
```
Input: "ì›ìë¡œ ë¶€í’ˆ"
  â†“
Tokenizer: [2, 1234, 5678, 3]  (ìˆ«ìë¡œ ë³€í™˜)
  â†“
Embedding Layer: ê° ìˆ«ìë¥¼ 768ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
  â†“
Transformer Encoder (12ì¸µ):
  - Self-Attention: ë‹¨ì–´ ê°„ ê´€ê³„ í•™ìŠµ
  - Feed-Forward: ë¹„ì„ í˜• ë³€í™˜
  â†“
Classification Head: 768ì°¨ì› â†’ 2ì°¨ì› (2ê°œ í´ë˜ìŠ¤)
  â†“
Output: logits [batch_size, 2]
```

**ë³€ìˆ˜:**
- `tokenizer`: í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì(í† í°)ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬
- `model`: KoBERT ì‹ ê²½ë§ ëª¨ë¸ (ì•½ 1ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°)

---

### 2.4 Dataset í´ë˜ìŠ¤ ìƒì„± (54-71ë²ˆ ì¤„)

```python
class StrategicItemDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        # ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— í† í¬ë‚˜ì´ì§•
        self.encodings = tokenizer(
            texts,
            truncation=True,      # 128ì ë„˜ìœ¼ë©´ ìë¥´ê¸°
            padding=True,         # ì§§ìœ¼ë©´ íŒ¨ë”© ì¶”ê°€
            max_length=max_length,
            return_tensors='pt'   # PyTorch í…ì„œë¡œ ë°˜í™˜
        )
        self.labels = labels

    def __getitem__(self, idx):
        # idxë²ˆì§¸ ìƒ˜í”Œ ë°˜í™˜
        item = {key: val[idx] for key, val in self.encodings.items()
                if key != 'token_type_ids'}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
```

**í† í¬ë‚˜ì´ì§• ê³¼ì • ì˜ˆì‹œ:**

```python
# ì…ë ¥ í…ìŠ¤íŠ¸
text = "ì›ìë¡œ ë¶€í’ˆ"

# í† í¬ë‚˜ì´ì§• ê²°ê³¼
{
    'input_ids': tensor([  2, 1234, 5678, 3, 0, 0, ..., 0]),  # 128ê°œ
    'attention_mask': tensor([1, 1, 1, 1, 0, 0, ..., 0])      # 128ê°œ
}
```

**ë³€ìˆ˜ ì„¤ëª…:**
- `input_ids`: í† í° ID (2=ì‹œì‘, 3=ë, 0=íŒ¨ë”©)
- `attention_mask`: ì‹¤ì œ í† í°ì€ 1, íŒ¨ë”©ì€ 0
- `labels`: ì •ë‹µ ë¼ë²¨ (0 ë˜ëŠ” 1)

---

### 2.5 í•™ìŠµ ì„¤ì • (83-96ë²ˆ ì¤„)

```python
training_args = TrainingArguments(
    output_dir='../models/kobert-strategic',
    num_train_epochs=3,                    # 3ë²ˆ ë°˜ë³µ í•™ìŠµ
    per_device_train_batch_size=16,        # í•œ ë²ˆì— 16ê°œì”©
    warmup_steps=100,                      # ì²˜ìŒ 100ìŠ¤í…ì€ í•™ìŠµë¥  ì¦ê°€
    weight_decay=0.01,                     # ê³¼ì í•© ë°©ì§€
    evaluation_strategy="epoch",           # ì—í¬í¬ë§ˆë‹¤ í‰ê°€
)
```

**í•™ìŠµ ê³¼ì •:**
```
ì—í¬í¬ 1:
  ìŠ¤í… 1-114: ë°°ì¹˜ 1-114 í•™ìŠµ (1,830ê°œ / 16 â‰ˆ 114)
  ê²€ì¦: ì •í™•ë„ ê³„ì‚°
ì—í¬í¬ 2:
  ìŠ¤í… 115-228: ë‹¤ì‹œ ì²˜ìŒë¶€í„° í•™ìŠµ
  ê²€ì¦: ì •í™•ë„ ê³„ì‚°
ì—í¬í¬ 3:
  ìŠ¤í… 229-342: ë§ˆì§€ë§‰ í•™ìŠµ
  ê²€ì¦: ì •í™•ë„ ê³„ì‚° â†’ ìµœì¢… 70.09%
```

---

### 2.6 í•™ìŠµ ì‹¤í–‰ (117ë²ˆ ì¤„)

```python
trainer.train()
```

**ë‚´ë¶€ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼:**

```python
for epoch in range(3):
    for batch in train_dataloader:
        # 1. Forward pass
        outputs = model(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            labels=batch['labels']
        )

        # outputs.loss: Cross-Entropy Loss ê³„ì‚°
        # loss = -log(P(ì •ë‹µ í´ë˜ìŠ¤))

        # 2. Backward pass
        loss.backward()  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°

        # 3. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        optimizer.step()  # ê°€ì¤‘ì¹˜ ì¡°ì •
        optimizer.zero_grad()
```

**Loss ê³„ì‚° ì˜ˆì‹œ:**
```
ì˜ˆì¸¡: [0.2, 0.8]  (ë¹„ì „ëµë¬¼ì 20%, ì „ëµë¬¼ì 80%)
ì •ë‹µ: 1 (ì „ëµë¬¼ì)

Cross-Entropy Loss = -log(0.8) = 0.223

ë§Œì•½ ì˜ˆì¸¡ì´ í‹€ë ¸ë‹¤ë©´:
ì˜ˆì¸¡: [0.9, 0.1]
ì •ë‹µ: 1
Loss = -log(0.1) = 2.303 (ë†’ì€ ì†ì‹¤)
```

---

### 2.7 ëª¨ë¸ ì €ì¥ (128-131ë²ˆ ì¤„)

```python
final_model_dir = '../models/kobert-strategic-final'
model.save_pretrained(final_model_dir)
tokenizer.save_pretrained(final_model_dir)
```

**ì €ì¥ë˜ëŠ” íŒŒì¼:**
- `config.json`: ëª¨ë¸ ì„¤ì •
- `model.safetensors`: í•™ìŠµëœ ê°€ì¤‘ì¹˜ (ì•½ 400MB)
- `tokenizer_config.json`: í† í¬ë‚˜ì´ì € ì„¤ì •
- `vocab.txt`: ë‹¨ì–´ ì‚¬ì „ (ì•½ 8,000ê°œ ë‹¨ì–´)

---

## 3. ë°±ì—”ë“œ ì˜ˆì¸¡ ì„œë²„

### íŒŒì¼: `backend/app.py`

---

### 3.1 ëª¨ë¸ ë¡œë“œ (19-25ë²ˆ ì¤„)

```python
model_path = "../models/kobert-strategic-final"
model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)
model.eval()  # í‰ê°€ ëª¨ë“œ (Dropout ë¹„í™œì„±í™”)
```

**ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ ì‹¤í–‰:**
- ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ (ì•½ 400MB RAM ì‚¬ìš©)
- GPUê°€ ìˆìœ¼ë©´ GPUë¡œ, ì—†ìœ¼ë©´ CPUë¡œ ì‹¤í–‰

---

### 3.2 ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸ (32-93ë²ˆ ì¤„)

#### ì…ë ¥ ë°ì´í„° êµ¬ì¡° (28-29ë²ˆ ì¤„)

```python
class PredictRequest(BaseModel):
    text: str  # ì˜ˆ: "ì›ìë¡œ ë¶€í’ˆ ë° ëƒ‰ê° ì‹œìŠ¤í…œ"
```

#### í•¨ìˆ˜ í˜¸ì¶œ íë¦„

```
í´ë¼ì´ì–¸íŠ¸ ìš”ì²­:
POST http://localhost:8000/predict
Body: { "text": "ì›ìë¡œ ë¶€í’ˆ" }
  â†“
FastAPI: PredictRequest ê°ì²´ ìƒì„±
  â†“
predict(request) í•¨ìˆ˜ ì‹¤í–‰
```

---

#### 3.2.1 í† í¬ë‚˜ì´ì§• (36-42ë²ˆ ì¤„)

```python
inputs = tokenizer(
    request.text,
    return_tensors="pt",      # PyTorch í…ì„œë¡œ ë°˜í™˜
    max_length=128,
    padding="max_length",      # í•­ìƒ 128ë¡œ íŒ¨ë”©
    truncation=True
)
```

**ì˜ˆì‹œ: "ì›ìë¡œ ë¶€í’ˆ" í† í¬ë‚˜ì´ì§•**

```python
# ì…ë ¥
request.text = "ì›ìë¡œ ë¶€í’ˆ"

# ì¶œë ¥ (inputs ë”•ì…”ë„ˆë¦¬)
{
    'input_ids': tensor([[2, 1234, 5678, 3, 0, 0, ..., 0]]),  # shape: [1, 128]
    'attention_mask': tensor([[1, 1, 1, 1, 0, 0, ..., 0]]),   # shape: [1, 128]
    'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, ..., 0]])    # (ì‚¬ìš© ì•ˆ í•¨)
}
```

**ë³€ìˆ˜ ì„¤ëª…:**
- `input_ids`: ê° í† í°ì˜ ID (ì •ìˆ˜)
  - 2: [CLS] í† í° (ì‹œì‘)
  - 1234: "ì›ìë¡œ"ì˜ ID
  - 5678: "ë¶€í’ˆ"ì˜ ID
  - 3: [SEP] í† í° (ë)
  - 0: [PAD] í† í° (íŒ¨ë”©)

- `attention_mask`: ì–´ëŠ í† í°ì´ ì‹¤ì œ ë‹¨ì–´ì¸ì§€ í‘œì‹œ
  - 1: ì‹¤ì œ í† í°
  - 0: íŒ¨ë”© (ë¬´ì‹œ)

---

#### 3.2.2 ëª¨ë¸ ì˜ˆì¸¡ (45-58ë²ˆ ì¤„)

```python
with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì•ˆ í•¨ (ì˜ˆì¸¡ë§Œ)
    # í•„ìš”í•œ ì…ë ¥ë§Œ ì¶”ì¶œ
    model_inputs = {
        'input_ids': inputs['input_ids'],
        'attention_mask': inputs['attention_mask']
    }

    # ëª¨ë¸ ì‹¤í–‰
    outputs = model(**model_inputs)
    logits = outputs.logits  # shape: [1, 2]

    # Softmaxë¡œ í™•ë¥  ê³„ì‚°
    probs = F.softmax(logits, dim=1)[0]
    prob_non_strategic = probs[0].item()  # ë¹„ì „ëµë¬¼ì í™•ë¥ 
    prob_strategic = probs[1].item()      # ì „ëµë¬¼ì í™•ë¥ 
```

**ëª¨ë¸ ë‚´ë¶€ ê³„ì‚° ê³¼ì •:**

```
Input: input_ids [1, 128], attention_mask [1, 128]
  â†“
Embedding Layer: [1, 128] â†’ [1, 128, 768]
  (ê° í† í°ì„ 768ì°¨ì› ë²¡í„°ë¡œ)
  â†“
Transformer Layer 1-12: [1, 128, 768] â†’ [1, 128, 768]
  ê° ì¸µë§ˆë‹¤:
    1. Self-Attention: ë‹¨ì–´ ê°„ ê´€ê³„ ê³„ì‚°
    2. Feed-Forward: ë¹„ì„ í˜• ë³€í™˜
  â†“
Pooling: [1, 128, 768] â†’ [1, 768]
  (ì²« ë²ˆì§¸ í† í° [CLS]ì˜ ë²¡í„°ë§Œ ì¶”ì¶œ)
  â†“
Classification Head: [1, 768] â†’ [1, 2]
  Linear(768, 2): ê°€ì¤‘ì¹˜ í–‰ë ¬ ê³±ì…ˆ
  â†“
Logits: [1, 2]
  ì˜ˆ: [[-2.5, 3.2]]
```

---

#### 3.2.3 Softmax ê³„ì‚° (ìˆ˜í•™)

```python
probs = F.softmax(logits, dim=1)[0]
```

**Softmax ê³µì‹:**

```
logits = [logit_0, logit_1]

P(class_i) = exp(logit_i) / (exp(logit_0) + exp(logit_1))
```

**êµ¬ì²´ì  ê³„ì‚° ì˜ˆì‹œ:**

```python
# logits (ëª¨ë¸ ì›ì‹œ ì¶œë ¥)
logits = [[-2.5, 3.2]]

# Softmax ê³„ì‚°
exp_0 = exp(-2.5) = 0.0821
exp_1 = exp(3.2) = 24.53

sum_exp = 0.0821 + 24.53 = 24.61

prob_0 = 0.0821 / 24.61 = 0.0033 (0.33%)
prob_1 = 24.53 / 24.61 = 0.9967 (99.67%)
```

**ê²°ê³¼:**
```python
probs = [0.0033, 0.9967]
prob_non_strategic = 0.0033  # 0.33%
prob_strategic = 0.9967      # 99.67%
```

---

#### 3.2.4 ê²°ê³¼ ë°˜í™˜ (61-84ë²ˆ ì¤„)

```python
# ì „ëµë¬¼ì ì—¬ë¶€ íŒë‹¨
is_strategic = prob_strategic > 0.5
confidence = prob_strategic if is_strategic else prob_non_strategic

# ì„ì‹œ ECCN/Class (ëœë¤)
import random
eccn = random.choice(['0A001', '0E001', '1C234', '2B231']) if is_strategic else 'N/A'
class_type = random.choice(['E1', 'E2', 'E3', 'A', 'B']) if is_strategic else 'N/A'

# ì„¤ëª… ìƒì„±
explanation = (
    f"KoBERT ë¶„ì„ ê²°ê³¼, ì „ëµë¬¼ìë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ {confidence*100:.1f}%ì…ë‹ˆë‹¤."
    if is_strategic
    else f"KoBERT ë¶„ì„ ê²°ê³¼, ì¼ë°˜ ìƒì—…ìš© í’ˆëª©ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤. (ì‹ ë¢°ë„: {confidence*100:.1f}%)"
)

# JSON ë°˜í™˜
return {
    "isStrategic": is_strategic,    # True or False
    "confidence": confidence * 100,  # 99.67
    "eccn": eccn,                    # "1C234"
    "classType": class_type,         # "E2"
    "explanation": explanation       # "KoBERT ë¶„ì„ ê²°ê³¼..."
}
```

**ë°˜í™˜ ì˜ˆì‹œ:**
```json
{
  "isStrategic": true,
  "confidence": 99.67,
  "eccn": "1C234",
  "classType": "E2",
  "explanation": "KoBERT ë¶„ì„ ê²°ê³¼, ì „ëµë¬¼ìë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ 99.7%ì…ë‹ˆë‹¤."
}
```

---

## 4. í”„ë¡ íŠ¸ì—”ë“œ í†µì‹ 

### íŒŒì¼: `frontend/src/services/kobertPrediction.ts`

---

### 4.1 API í˜¸ì¶œ í•¨ìˆ˜ (5-38ë²ˆ ì¤„)

```typescript
const API_URL = 'http://localhost:8000';

export async function predictStrategicItem(text: string): Promise<{
  isStrategic: boolean;
  confidence: number;
  eccn: string;
  classType: string;
  explanation: string;
}> {
  // ë°±ì—”ë“œì— HTTP POST ìš”ì²­
  const response = await fetch(`${API_URL}/predict`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ text })  // { text: "ì›ìë¡œ ë¶€í’ˆ" }
  });

  if (!response.ok) {
    throw new Error(`API request failed: ${response.statusText}`);
  }

  // JSON ì‘ë‹µì„ ê°ì²´ë¡œ íŒŒì‹±
  const result = await response.json();
  return result;
}
```

**í˜¸ì¶œ ì˜ˆì‹œ:**

```typescript
// PredictionPage.tsxì—ì„œ í˜¸ì¶œ
const queryText = "ì›ìë¡œ ë¶€í’ˆ ë° ëƒ‰ê° ì‹œìŠ¤í…œ";
const result = await predictStrategicItem(queryText);

// result ë³€ìˆ˜ì— ì €ì¥ëœ ê°’
{
  isStrategic: true,
  confidence: 99.67,
  eccn: "1C234",
  classType: "E2",
  explanation: "KoBERT ë¶„ì„ ê²°ê³¼..."
}
```

---

## 5. TF-IDF ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰

### íŒŒì¼: `frontend/src/utils/tfidf.ts`

ì´ íŒŒì¼ì€ **ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê³¼ê±° ë°ì´í„° ì¤‘ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ëŠ”** ê¸°ëŠ¥ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

---

### 5.1 í† í¬ë‚˜ì´ì§• (9-20ë²ˆ ì¤„)

```typescript
export const tokenize = (text: string): string[] => {
  if (!text) return [];

  // í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
  const cleaned = text.toLowerCase()
    .replace(/[^\wã„±-ã…ê°€-í£a-z0-9\s]/g, ' ');

  // ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
  const tokens = cleaned.split(/\s+/).filter(t => t.length > 1);

  return tokens;
};
```

**ì˜ˆì‹œ:**

```typescript
// ì…ë ¥
text = "ì›ìë¡œ ë¶€í’ˆ ë° ëƒ‰ê° ì‹œìŠ¤í…œ!!!"

// ê³¼ì •
1. ì†Œë¬¸ì ë³€í™˜: "ì›ìë¡œ ë¶€í’ˆ ë° ëƒ‰ê° ì‹œìŠ¤í…œ!!!"
2. íŠ¹ìˆ˜ë¬¸ì ì œê±°: "ì›ìë¡œ ë¶€í’ˆ ë° ëƒ‰ê° ì‹œìŠ¤í…œ   "
3. ë¶„ë¦¬: ["ì›ìë¡œ", "ë¶€í’ˆ", "ë°", "ëƒ‰ê°", "ì‹œìŠ¤í…œ"]
4. ê¸¸ì´ í•„í„° (>1): ["ì›ìë¡œ", "ë¶€í’ˆ", "ë°", "ëƒ‰ê°", "ì‹œìŠ¤í…œ"]

// ì¶œë ¥
["ì›ìë¡œ", "ë¶€í’ˆ", "ë°", "ëƒ‰ê°", "ì‹œìŠ¤í…œ"]
```

---

### 5.2 ë¶ˆìš©ì–´ ì œê±° (23-33ë²ˆ ì¤„)

```typescript
const stopWords = new Set([
  'the', 'a', 'an', 'and', 'or', 'but', ...
  'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', ...
]);

export const removeStopWords = (tokens: string[]): string[] => {
  return tokens.filter(token => !stopWords.has(token));
};
```

**ì˜ˆì‹œ:**

```typescript
// ì…ë ¥
tokens = ["ì›ìë¡œ", "ë¶€í’ˆ", "ë°", "ëƒ‰ê°", "ì‹œìŠ¤í…œ"]

// "ë°"ì€ ë¶ˆìš©ì–´
removeStopWords(tokens)

// ì¶œë ¥
["ì›ìë¡œ", "ë¶€í’ˆ", "ëƒ‰ê°", "ì‹œìŠ¤í…œ"]
```

---

### 5.3 TF (Term Frequency) ê³„ì‚° (36-50ë²ˆ ì¤„)

```typescript
export const calculateTF = (tokens: string[]): Map<string, number> => {
  const tf = new Map<string, number>();
  const totalTerms = tokens.length;

  // ê° ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
  tokens.forEach(token => {
    tf.set(token, (tf.get(token) || 0) + 1);
  });

  // ì •ê·œí™” (ë¹ˆë„ / ì „ì²´ ë‹¨ì–´ ìˆ˜)
  tf.forEach((count, term) => {
    tf.set(term, count / totalTerms);
  });

  return tf;
};
```

**TF ê³µì‹:**
```
TF(ë‹¨ì–´) = ë¬¸ì„œ ë‚´ ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜ / ë¬¸ì„œì˜ ì „ì²´ ë‹¨ì–´ ìˆ˜
```

**ì˜ˆì‹œ:**

```typescript
// ì…ë ¥
tokens = ["ì›ìë¡œ", "ë¶€í’ˆ", "ì›ìë¡œ", "ì‹œìŠ¤í…œ"]
totalTerms = 4

// ë¹ˆë„ ê³„ì‚°
"ì›ìë¡œ": 2ë²ˆ
"ë¶€í’ˆ": 1ë²ˆ
"ì‹œìŠ¤í…œ": 1ë²ˆ

// ì •ê·œí™”
TF("ì›ìë¡œ") = 2/4 = 0.5
TF("ë¶€í’ˆ") = 1/4 = 0.25
TF("ì‹œìŠ¤í…œ") = 1/4 = 0.25

// ì¶œë ¥
Map {
  "ì›ìë¡œ" => 0.5,
  "ë¶€í’ˆ" => 0.25,
  "ì‹œìŠ¤í…œ" => 0.25
}
```

---

### 5.4 IDF (Inverse Document Frequency) ê³„ì‚° (53-73ë²ˆ ì¤„)

```typescript
export const calculateIDF = (documents: string[][]): Map<string, number> => {
  const idf = new Map<string, number>();
  const totalDocs = documents.length;

  // ê° ë‹¨ì–´ê°€ ëª‡ ê°œ ë¬¸ì„œì— ë“±ì¥í•˜ëŠ”ì§€ ê³„ì‚°
  const docFrequency = new Map<string, number>();

  documents.forEach(doc => {
    const uniqueTerms = new Set(doc);
    uniqueTerms.forEach(term => {
      docFrequency.set(term, (docFrequency.get(term) || 0) + 1);
    });
  });

  // IDF = log(ì „ì²´ ë¬¸ì„œ ìˆ˜ / ë‹¨ì–´ê°€ ë“±ì¥í•œ ë¬¸ì„œ ìˆ˜)
  docFrequency.forEach((freq, term) => {
    idf.set(term, Math.log(totalDocs / freq));
  });

  return idf;
};
```

**IDF ê³µì‹:**
```
IDF(ë‹¨ì–´) = log(ì „ì²´ ë¬¸ì„œ ìˆ˜ / ë‹¨ì–´ê°€ ë“±ì¥í•œ ë¬¸ì„œ ìˆ˜)
```

**ì˜ˆì‹œ:**

```typescript
// ì…ë ¥: 3ê°œ ë¬¸ì„œ
documents = [
  ["ì›ìë¡œ", "ë¶€í’ˆ"],      // ë¬¸ì„œ 1
  ["ì›ìë¡œ", "ì‹œìŠ¤í…œ"],    // ë¬¸ì„œ 2
  ["ì»´í“¨í„°", "ë¶€í’ˆ"]       // ë¬¸ì„œ 3
]
totalDocs = 3

// ë¬¸ì„œ ë¹ˆë„ ê³„ì‚°
"ì›ìë¡œ": 2ê°œ ë¬¸ì„œì— ë“±ì¥ (ë¬¸ì„œ 1, 2)
"ë¶€í’ˆ": 2ê°œ ë¬¸ì„œì— ë“±ì¥ (ë¬¸ì„œ 1, 3)
"ì‹œìŠ¤í…œ": 1ê°œ ë¬¸ì„œì— ë“±ì¥ (ë¬¸ì„œ 2)
"ì»´í“¨í„°": 1ê°œ ë¬¸ì„œì— ë“±ì¥ (ë¬¸ì„œ 3)

// IDF ê³„ì‚°
IDF("ì›ìë¡œ") = log(3/2) = log(1.5) = 0.405
IDF("ë¶€í’ˆ") = log(3/2) = 0.405
IDF("ì‹œìŠ¤í…œ") = log(3/1) = log(3) = 1.099
IDF("ì»´í“¨í„°") = log(3/1) = 1.099

// ì¶œë ¥
Map {
  "ì›ìë¡œ" => 0.405,
  "ë¶€í’ˆ" => 0.405,
  "ì‹œìŠ¤í…œ" => 1.099,
  "ì»´í“¨í„°" => 1.099
}
```

**ì˜ë¯¸:**
- IDFê°€ ë‚®ìŒ (0.405): ë§ì€ ë¬¸ì„œì— ë“±ì¥ â†’ ì¼ë°˜ì ì¸ ë‹¨ì–´
- IDFê°€ ë†’ìŒ (1.099): ì ì€ ë¬¸ì„œì— ë“±ì¥ â†’ íŠ¹ë³„í•œ ë‹¨ì–´

---

### 5.5 TF-IDF ê³„ì‚° (76-88ë²ˆ ì¤„)

```typescript
export const calculateTFIDF = (
  tf: Map<string, number>,
  idf: Map<string, number>
): Map<string, number> => {
  const tfidf = new Map<string, number>();

  tf.forEach((tfValue, term) => {
    const idfValue = idf.get(term) || 0;
    tfidf.set(term, tfValue * idfValue);
  });

  return tfidf;
};
```

**TF-IDF ê³µì‹:**
```
TF-IDF(ë‹¨ì–´) = TF(ë‹¨ì–´) Ã— IDF(ë‹¨ì–´)
```

**ì˜ˆì‹œ:**

```typescript
// ì…ë ¥
tf = Map {
  "ì›ìë¡œ" => 0.5,
  "ë¶€í’ˆ" => 0.25
}

idf = Map {
  "ì›ìë¡œ" => 0.405,
  "ë¶€í’ˆ" => 0.405
}

// ê³„ì‚°
TF-IDF("ì›ìë¡œ") = 0.5 Ã— 0.405 = 0.2025
TF-IDF("ë¶€í’ˆ") = 0.25 Ã— 0.405 = 0.1013

// ì¶œë ¥
Map {
  "ì›ìë¡œ" => 0.2025,
  "ë¶€í’ˆ" => 0.1013
}
```

---

### 5.6 ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (91-116ë²ˆ ì¤„)

```typescript
export const cosineSimilarity = (
  vec1: Map<string, number>,
  vec2: Map<string, number>
): number => {
  let dotProduct = 0;
  let norm1 = 0;
  let norm2 = 0;

  const allTerms = new Set([...vec1.keys(), ...vec2.keys()]);

  allTerms.forEach(term => {
    const v1 = vec1.get(term) || 0;
    const v2 = vec2.get(term) || 0;

    dotProduct += v1 * v2;
    norm1 += v1 * v1;
    norm2 += v2 * v2;
  });

  if (norm1 === 0 || norm2 === 0) return 0;

  return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
};
```

**ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³µì‹:**

```
            A Â· B
cos(Î¸) = â”€â”€â”€â”€â”€â”€â”€â”€â”€
          |A| |B|

A Â· B = Î£(a_i Ã— b_i)  (ë‚´ì )
|A| = âˆš(Î£ a_iÂ²)       (í¬ê¸°)
```

**ì˜ˆì‹œ:**

```typescript
// ë‘ ë¬¸ì„œì˜ TF-IDF ë²¡í„°
vec1 = Map {  // ì¿¼ë¦¬: "ì›ìë¡œ ë¶€í’ˆ"
  "ì›ìë¡œ" => 0.3,
  "ë¶€í’ˆ" => 0.2
}

vec2 = Map {  // ë¬¸ì„œ 1: "ì›ìë¡œ ì‹œìŠ¤í…œ"
  "ì›ìë¡œ" => 0.4,
  "ì‹œìŠ¤í…œ" => 0.1
}

// ëª¨ë“  ë‹¨ì–´: ["ì›ìë¡œ", "ë¶€í’ˆ", "ì‹œìŠ¤í…œ"]
// ë²¡í„°ë¡œ í‘œí˜„:
// vec1 = [0.3, 0.2, 0]
// vec2 = [0.4, 0, 0.1]

// ë‚´ì  ê³„ì‚°
dotProduct = (0.3Ã—0.4) + (0.2Ã—0) + (0Ã—0.1)
           = 0.12 + 0 + 0
           = 0.12

// í¬ê¸° ê³„ì‚°
norm1 = âˆš(0.3Â² + 0.2Â² + 0Â²) = âˆš(0.09 + 0.04) = âˆš0.13 = 0.361
norm2 = âˆš(0.4Â² + 0Â² + 0.1Â²) = âˆš(0.16 + 0.01) = âˆš0.17 = 0.412

// ì½”ì‚¬ì¸ ìœ ì‚¬ë„
similarity = 0.12 / (0.361 Ã— 0.412)
           = 0.12 / 0.149
           = 0.806  (80.6%)

// ì¶œë ¥
0.806
```

**ì˜ë¯¸:**
- 1.0: ì™„ì „íˆ ë™ì¼
- 0.8: ë§¤ìš° ìœ ì‚¬
- 0.5: ì–´ëŠ ì •ë„ ìœ ì‚¬
- 0.0: ì „í˜€ ë‹¤ë¦„

---

### 5.7 ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ë©”ì¸ í•¨ìˆ˜ (119-163ë²ˆ ì¤„)

```typescript
export const findSimilarDocuments = (
  query: string,
  documents: any[],
  topN: number = 5
): TFIDFResult[] => {
  // 1. ì¿¼ë¦¬ í† í¬ë‚˜ì´ì§•
  const queryTokens = removeStopWords(tokenize(query));

  // 2. ëª¨ë“  ë¬¸ì„œ í† í¬ë‚˜ì´ì§•
  const documentTexts = documents.map(doc => {
    const combined = `${doc.title || ''} ${doc.description || ''} ${doc.purpose || ''} ${doc.application || ''}`;
    return removeStopWords(tokenize(combined));
  });

  // 3. IDF ê³„ì‚° (ì „ì²´ ë¬¸ì„œ ê¸°ì¤€)
  const idf = calculateIDF([queryTokens, ...documentTexts]);

  // 4. ì¿¼ë¦¬ TF-IDF ê³„ì‚°
  const queryTF = calculateTF(queryTokens);
  const queryTFIDF = calculateTFIDF(queryTF, idf);

  // 5. ê° ë¬¸ì„œì™€ ìœ ì‚¬ë„ ê³„ì‚°
  const similarities: TFIDFResult[] = documents.map((doc, index) => {
    const docTokens = documentTexts[index];
    const docTF = calculateTF(docTokens);
    const docTFIDF = calculateTFIDF(docTF, idf);

    const similarity = cosineSimilarity(queryTFIDF, docTFIDF);

    return {
      itemId: index,
      similarity: similarity
    };
  });

  // 6. ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œ ë°˜í™˜
  return similarities
    .filter(s => s.similarity > 0)
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, topN);
};
```

**ì „ì²´ ê³¼ì • ì˜ˆì‹œ:**

```typescript
// ì…ë ¥
query = "ì›ìë¡œ ëƒ‰ê° ì‹œìŠ¤í…œ"
documents = [
  { title: "ì›ìë¡œ ë¶€í’ˆ", description: "ëƒ‰ê° ì¥ì¹˜" },     // ë¬¸ì„œ 0
  { title: "ì»´í“¨í„°", description: "ë©”ëª¨ë¦¬" },             // ë¬¸ì„œ 1
  { title: "ì›ìë¡œ", description: "ì‹œìŠ¤í…œ ì„¤ê³„" }         // ë¬¸ì„œ 2
]

// 1. ì¿¼ë¦¬ í† í¬ë‚˜ì´ì§•
queryTokens = ["ì›ìë¡œ", "ëƒ‰ê°", "ì‹œìŠ¤í…œ"]

// 2. ë¬¸ì„œ í† í¬ë‚˜ì´ì§•
documentTexts = [
  ["ì›ìë¡œ", "ë¶€í’ˆ", "ëƒ‰ê°", "ì¥ì¹˜"],    // ë¬¸ì„œ 0
  ["ì»´í“¨í„°", "ë©”ëª¨ë¦¬"],                   // ë¬¸ì„œ 1
  ["ì›ìë¡œ", "ì‹œìŠ¤í…œ", "ì„¤ê³„"]           // ë¬¸ì„œ 2
]

// 3. IDF ê³„ì‚°
idf = {
  "ì›ìë¡œ": log(4/3) = 0.288,  // ì¿¼ë¦¬ + ë¬¸ì„œ 0, 2
  "ëƒ‰ê°": log(4/2) = 0.693,    // ì¿¼ë¦¬ + ë¬¸ì„œ 0
  "ì‹œìŠ¤í…œ": log(4/2) = 0.693,  // ì¿¼ë¦¬ + ë¬¸ì„œ 2
  "ë¶€í’ˆ": log(4/1) = 1.386,
  "ì¥ì¹˜": log(4/1) = 1.386,
  "ì»´í“¨í„°": log(4/1) = 1.386,
  "ë©”ëª¨ë¦¬": log(4/1) = 1.386,
  "ì„¤ê³„": log(4/1) = 1.386
}

// 4. ì¿¼ë¦¬ TF-IDF
queryTF = { "ì›ìë¡œ": 1/3, "ëƒ‰ê°": 1/3, "ì‹œìŠ¤í…œ": 1/3 }
queryTFIDF = {
  "ì›ìë¡œ": 0.096,
  "ëƒ‰ê°": 0.231,
  "ì‹œìŠ¤í…œ": 0.231
}

// 5. ê° ë¬¸ì„œ TF-IDF ë° ìœ ì‚¬ë„
ë¬¸ì„œ 0 TF-IDF: { "ì›ìë¡œ": 0.072, "ë¶€í’ˆ": 0.347, "ëƒ‰ê°": 0.173, "ì¥ì¹˜": 0.347 }
ìœ ì‚¬ë„ 0: cos(query, doc0) = 0.78  (ë†’ìŒ!)

ë¬¸ì„œ 1 TF-IDF: { "ì»´í“¨í„°": 0.693, "ë©”ëª¨ë¦¬": 0.693 }
ìœ ì‚¬ë„ 1: cos(query, doc1) = 0.0  (ë‚®ìŒ!)

ë¬¸ì„œ 2 TF-IDF: { "ì›ìë¡œ": 0.096, "ì‹œìŠ¤í…œ": 0.231, "ì„¤ê³„": 0.462 }
ìœ ì‚¬ë„ 2: cos(query, doc2) = 0.85  (ë§¤ìš° ë†’ìŒ!)

// 6. ì •ë ¬ ë° ë°˜í™˜
[
  { itemId: 2, similarity: 0.85 },
  { itemId: 0, similarity: 0.78 }
]
```

---

## 6. UI ë Œë”ë§

### íŒŒì¼: `frontend/src/pages/PredictionPage.tsx`

---

### 6.1 ìƒíƒœ ê´€ë¦¬ (12-14ë²ˆ ì¤„)

```typescript
const [predictionResult, setPredictionResult] = useState<any>(null);
const [similarCases, setSimilarCases] = useState<any[]>([]);
const [allData, setAllData] = useState<any[]>([]);
```

**ë³€ìˆ˜ ì„¤ëª…:**
- `predictionResult`: ë°±ì—”ë“œë¡œë¶€í„° ë°›ì€ ì˜ˆì¸¡ ê²°ê³¼
- `similarCases`: TF-IDFë¡œ ì°¾ì€ ìœ ì‚¬ ë¬¸ì„œë“¤
- `allData`: export_history.jsonì˜ ëª¨ë“  ê³¼ê±° ë°ì´í„°

---

### 6.2 handlePredict í•¨ìˆ˜ (31-73ë²ˆ ì¤„)

```typescript
const handlePredict = async (formData: any) => {
  // 1. ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ìƒì„±
  const queryText = `${formData.title} ${formData.description || ''} ${formData.purpose || ''} ${formData.application || ''}`;

  // 2. KoBERT ëª¨ë¸ë¡œ ì˜ˆì¸¡
  try {
    const result = await predictStrategicItem(queryText);
    setPredictionResult(result);  // ìƒíƒœ ì—…ë°ì´íŠ¸ â†’ UI ìë™ ë Œë”ë§
  } catch (error) {
    console.error('Prediction error:', error);
    message.error('ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
  }

  // 3. ìœ ì‚¬ ì‚¬ë¡€ ì°¾ê¸°
  if (queryText.trim().length < 3) {
    setSimilarCases([]);
    message.warning('ë” êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.');
    return;
  }

  const similarResults = findSimilarDocuments(queryText, allData, 5);
  const filteredResults = similarResults.filter(result => result.similarity > 0.05);

  const cases = filteredResults.map(result => ({
    ...allData[result.itemId],
    similarity: result.similarity,
    rank: filteredResults.indexOf(result) + 1
  }));

  setSimilarCases(cases);  // ìƒíƒœ ì—…ë°ì´íŠ¸
  message.success(`${cases.length}ê°œì˜ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.`);
};
```

**ë³€ìˆ˜ íë¦„:**

```
1. ì‚¬ìš©ì ì…ë ¥ (PredictionForm)
   formData = {
     title: "ì›ìë¡œ ë¶€í’ˆ",
     description: "ëƒ‰ê° ì‹œìŠ¤í…œ",
     purpose: "ë°œì „ì†Œ ì‚¬ìš©",
     application: ""
   }

2. queryText ìƒì„±
   queryText = "ì›ìë¡œ ë¶€í’ˆ ëƒ‰ê° ì‹œìŠ¤í…œ ë°œì „ì†Œ ì‚¬ìš© "

3. ë°±ì—”ë“œ API í˜¸ì¶œ
   result = {
     isStrategic: true,
     confidence: 99.67,
     eccn: "1C234",
     classType: "E2",
     explanation: "..."
   }

4. TF-IDF ê²€ìƒ‰
   similarResults = [
     { itemId: 42, similarity: 0.85 },
     { itemId: 17, similarity: 0.72 }
   ]

5. ìƒì„¸ ì •ë³´ ì¶”ê°€
   cases = [
     {
       ...allData[42],  // ì›ë˜ ë¬¸ì„œ ì •ë³´
       similarity: 0.85,
       rank: 1
     },
     {
       ...allData[17],
       similarity: 0.72,
       rank: 2
     }
   ]

6. ìƒíƒœ ì—…ë°ì´íŠ¸
   setPredictionResult(result)  â†’ PredictionResult ì»´í¬ë„ŒíŠ¸ ë Œë”ë§
   setSimilarCases(cases)        â†’ SimilarCasesList ì»´í¬ë„ŒíŠ¸ ë Œë”ë§
```

---

### 6.3 PredictionResult ì»´í¬ë„ŒíŠ¸

### íŒŒì¼: `frontend/src/components/prediction/PredictionResult.tsx`

```typescript
const PredictionResult: React.FC<PredictionResultProps> = ({ result }) => {
  if (!result) {
    return <Empty />;  // ê²°ê³¼ ì—†ìœ¼ë©´ ë¹ˆ í™”ë©´
  }

  return (
    <Card title="ì˜ˆì¸¡ ê²°ê³¼">
      {/* ì „ëµë¬¼ì ì—¬ë¶€ í‘œì‹œ */}
      <Tag color={result.isStrategic ? 'red' : 'green'}>
        {result.isStrategic ? 'ì „ëµë¬¼ì' : 'ë¹„ì „ëµë¬¼ì'}
      </Tag>

      {/* ì‹ ë¢°ë„ Progress Bar */}
      <Progress percent={result.confidence} />

      {/* ìƒì„¸ ì •ë³´ */}
      <Descriptions>
        <Descriptions.Item label="ECCN">
          {result.eccn}
        </Descriptions.Item>
        <Descriptions.Item label="ì„¤ëª…">
          {result.explanation}
        </Descriptions.Item>
      </Descriptions>
    </Card>
  );
};
```

**ë Œë”ë§ ì˜ˆì‹œ:**

```
ì…ë ¥: result = {
  isStrategic: true,
  confidence: 99.67,
  eccn: "1C234",
  explanation: "KoBERT ë¶„ì„ ê²°ê³¼..."
}

ì¶œë ¥ (í™”ë©´):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì˜ˆì¸¡ ê²°ê³¼                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [ì „ëµë¬¼ì] ğŸ”´                    â”‚
â”‚                                  â”‚
â”‚ ì‹ ë¢°ë„: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 99.7%      â”‚
â”‚                                  â”‚
â”‚ ECCN: 1C234                      â”‚
â”‚ ì„¤ëª…: KoBERT ë¶„ì„ ê²°ê³¼...        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. ì „ì²´ ë°ì´í„° íë¦„ ìš”ì•½

### 7.1 ì‹œê°„ìˆœ íë¦„

```
[T0] ì‚¬ìš©ìê°€ "ì›ìë¡œ ë¶€í’ˆ"ì„ ì…ë ¥í•˜ê³  ë²„íŠ¼ í´ë¦­
  â†“
[T1] PredictionForm â†’ handlePredict() í˜¸ì¶œ
  ë³€ìˆ˜: formData = { title: "ì›ìë¡œ ë¶€í’ˆ", description: "..." }
  â†“
[T2] queryText ìƒì„±
  ë³€ìˆ˜: queryText = "ì›ìë¡œ ë¶€í’ˆ ëƒ‰ê° ì‹œìŠ¤í…œ"
  â†“
[T3] predictStrategicItem(queryText) í˜¸ì¶œ (kobertPrediction.ts)
  â†“
[T4] HTTP POST â†’ ë°±ì—”ë“œ (http://localhost:8000/predict)
  Body: { text: "ì›ìë¡œ ë¶€í’ˆ ëƒ‰ê° ì‹œìŠ¤í…œ" }
  â†“
[T5] ë°±ì—”ë“œ: tokenizer(text)
  ë³€ìˆ˜: inputs = {
    input_ids: [[2, 1234, 5678, ...]],
    attention_mask: [[1, 1, 1, ...]]
  }
  â†“
[T6] ë°±ì—”ë“œ: model(inputs)
  ë³€ìˆ˜: logits = [[-2.5, 3.2]]
  â†“
[T7] ë°±ì—”ë“œ: softmax(logits)
  ë³€ìˆ˜: probs = [0.0033, 0.9967]
  â†“
[T8] ë°±ì—”ë“œ: JSON ì‘ë‹µ ë°˜í™˜
  {
    isStrategic: true,
    confidence: 99.67,
    eccn: "1C234",
    ...
  }
  â†“
[T9] í”„ë¡ íŠ¸ì—”ë“œ: setPredictionResult(result)
  â†’ React ìƒíƒœ ì—…ë°ì´íŠ¸
  â†“
[T10] PredictionResult ì»´í¬ë„ŒíŠ¸ ìë™ ë¦¬ë Œë”ë§
  â†’ í™”ë©´ì— ê²°ê³¼ í‘œì‹œ
  â†“
[T11] ë™ì‹œì— findSimilarDocuments(queryText, allData, 5) ì‹¤í–‰
  â†“
[T12] TF-IDF ê³„ì‚° ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
  ë³€ìˆ˜: similarities = [{ itemId: 42, similarity: 0.85 }, ...]
  â†“
[T13] setSimilarCases(cases)
  â†’ React ìƒíƒœ ì—…ë°ì´íŠ¸
  â†“
[T14] SimilarCasesList ì»´í¬ë„ŒíŠ¸ ìë™ ë¦¬ë Œë”ë§
  â†’ í™”ë©´ì— ìœ ì‚¬ ì‚¬ë¡€ í‘œì‹œ
```

---

### 7.2 í•µì‹¬ ë³€ìˆ˜ ì¶”ì í‘œ

| ë‹¨ê³„ | ë³€ìˆ˜ ì´ë¦„ | íƒ€ì… | ì˜ˆì‹œ ê°’ |
|------|----------|------|---------|
| ì‚¬ìš©ì ì…ë ¥ | `formData` | Object | `{ title: "ì›ìë¡œ ë¶€í’ˆ", description: "..." }` |
| ì¿¼ë¦¬ ìƒì„± | `queryText` | string | `"ì›ìë¡œ ë¶€í’ˆ ëƒ‰ê° ì‹œìŠ¤í…œ"` |
| í† í¬ë‚˜ì´ì§• | `inputs.input_ids` | Tensor | `[[2, 1234, 5678, 3, 0, ...]]` |
| ëª¨ë¸ ì¶œë ¥ | `logits` | Tensor | `[[-2.5, 3.2]]` |
| í™•ë¥  ê³„ì‚° | `probs` | Tensor | `[0.0033, 0.9967]` |
| ë°±ì—”ë“œ ì‘ë‹µ | `result` | Object | `{ isStrategic: true, confidence: 99.67, ... }` |
| React ìƒíƒœ | `predictionResult` | Object | (ë™ì¼) |
| TF-IDF | `queryTFIDF` | Map | `Map { "ì›ìë¡œ" => 0.096, ... }` |
| ìœ ì‚¬ë„ | `similarities` | Array | `[{ itemId: 42, similarity: 0.85 }, ...]` |
| React ìƒíƒœ | `similarCases` | Array | (ìƒì„¸ ì •ë³´ ì¶”ê°€ëœ ë°°ì—´) |

---

### 7.3 ìˆ˜í•™ ê³µì‹ ì´ì •ë¦¬

#### 1. Softmax (ë°±ì—”ë“œ)
```
P(class_i) = exp(logit_i) / Î£ exp(logit_j)
```

#### 2. Cross-Entropy Loss (í•™ìŠµ ì‹œ)
```
Loss = -log(P(ì •ë‹µ í´ë˜ìŠ¤))
```

#### 3. TF (Term Frequency)
```
TF(ë‹¨ì–´) = ë¬¸ì„œ ë‚´ ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜ / ë¬¸ì„œì˜ ì „ì²´ ë‹¨ì–´ ìˆ˜
```

#### 4. IDF (Inverse Document Frequency)
```
IDF(ë‹¨ì–´) = log(ì „ì²´ ë¬¸ì„œ ìˆ˜ / ë‹¨ì–´ê°€ ë“±ì¥í•œ ë¬¸ì„œ ìˆ˜)
```

#### 5. TF-IDF
```
TF-IDF(ë‹¨ì–´) = TF(ë‹¨ì–´) Ã— IDF(ë‹¨ì–´)
```

#### 6. ì½”ì‚¬ì¸ ìœ ì‚¬ë„
```
similarity = (A Â· B) / (|A| Ã— |B|)

A Â· B = Î£(a_i Ã— b_i)
|A| = âˆš(Î£ a_iÂ²)
```

---

## 8. ì¶”ê°€ ì„¤ëª…: ì™œ ì´ë ‡ê²Œ ë³µì¡í•œê°€?

### 8.1 ì™œ KoBERTë¥¼ ì‚¬ìš©í•˜ë‚˜?

**ì „í†µì  ë°©ë²• (í‚¤ì›Œë“œ ë§¤ì¹­):**
```python
if "ì›ìë¡œ" in text or "í•µ" in text:
    return "ì „ëµë¬¼ì"
```
**ë¬¸ì œ:** ë¬¸ë§¥ì„ ì´í•´ ëª»í•¨

**KoBERT ë°©ë²•:**
- "ì›ìë¡œ ëª¨í˜• ì¥ë‚œê°" â†’ ë¹„ì „ëµë¬¼ì (ì¥ë‚œê° ë¬¸ë§¥)
- "ì›ìë¡œ ëƒ‰ê° ì‹œìŠ¤í…œ" â†’ ì „ëµë¬¼ì (ì‹¤ì œ ë¶€í’ˆ ë¬¸ë§¥)

â†’ **ë¬¸ë§¥ì„ ì´í•´í•˜ê³  íŒë‹¨**

---

### 8.2 ì™œ TF-IDFë¥¼ ì‚¬ìš©í•˜ë‚˜?

**ë‹¨ìˆœ ë‹¨ì–´ ë§¤ì¹­:**
```python
if "ì›ìë¡œ" in query and "ì›ìë¡œ" in document:
    return True
```
**ë¬¸ì œ:** ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œ ë‹¨ì–´ì¸ì§€ ëª¨ë¦„

**TF-IDF:**
- "ì›ìë¡œ": IDF ë†’ìŒ â†’ ì¤‘ìš” (ì ì€ ë¬¸ì„œì— ë“±ì¥)
- "ì‹œìŠ¤í…œ": IDF ë‚®ìŒ â†’ ëœ ì¤‘ìš” (ë§ì€ ë¬¸ì„œì— ë“±ì¥)

â†’ **ì¤‘ìš”í•œ ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬**

---

### 8.3 ì™œ í”„ë¡ íŠ¸/ë°±ì—”ë“œë¥¼ ë¶„ë¦¬í•˜ë‚˜?

**ëª¨ë‘ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‹¤í–‰í•˜ë©´?**
- KoBERT ëª¨ë¸ (400MB)ì„ ë¸Œë¼ìš°ì €ì—ì„œ ë¡œë“œ?
- ëŠë¦¬ê³ , ë©”ëª¨ë¦¬ ë¶€ì¡±

**ë°±ì—”ë“œ ë¶„ë¦¬:**
- ì„œë²„ì—ì„œ GPUë¡œ ë¹ ë¥´ê²Œ ì‹¤í–‰
- í”„ë¡ íŠ¸ì—”ë“œëŠ” ê°€ë²¼ìš´ UIë§Œ ë‹´ë‹¹

---

## 9. ë””ë²„ê¹… ê°€ì´ë“œ

### 9.1 ì˜ˆì¸¡ ê²°ê³¼ê°€ ì´ìƒí•  ë•Œ

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
1. ë°±ì—”ë“œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ê°€? (`http://localhost:8000/health`)
2. ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ê°€? (`models/kobert-strategic-final/`)
3. ì½˜ì†”ì— ì—ëŸ¬ê°€ ìˆëŠ”ê°€? (F12)

**ë””ë²„ê¹…:**
```python
# backend/app.pyì— ì¶”ê°€
print(f"Input text: {request.text}")
print(f"Logits: {logits}")
print(f"Probs: {probs}")
```

---

### 9.2 ìœ ì‚¬ ì‚¬ë¡€ê°€ ì•ˆ ë‚˜ì˜¬ ë•Œ

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
1. `export_history.json` íŒŒì¼ì´ ìˆëŠ”ê°€?
2. ì¿¼ë¦¬ê°€ ë„ˆë¬´ ì§§ì€ê°€? (ìµœì†Œ 3ì)
3. ìœ ì‚¬ë„ ì„ê³„ê°’ì´ ë„ˆë¬´ ë†’ì€ê°€? (0.05)

**ë””ë²„ê¹…:**
```typescript
// PredictionPage.tsxì— ì¶”ê°€
console.log('Query tokens:', queryTokens);
console.log('Similarities:', similarResults);
```

---

## 10. ê²°ë¡ 

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ìˆ  ìŠ¤íƒì„ í†µí•©í•©ë‹ˆë‹¤:

1. **AI/ML**: KoBERT (Transformer), PyTorch
2. **ë°±ì—”ë“œ**: FastAPI, Python
3. **í”„ë¡ íŠ¸ì—”ë“œ**: React, TypeScript
4. **ì•Œê³ ë¦¬ì¦˜**: TF-IDF, ì½”ì‚¬ì¸ ìœ ì‚¬ë„
5. **í†µì‹ **: REST API, JSON

ëª¨ë“  ë¶€ë¶„ì´ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ **í…ìŠ¤íŠ¸ ì…ë ¥ â†’ AI ë¶„ì„ â†’ ê²°ê³¼ í‘œì‹œ**ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

---

**ì‘ì„±ì¼**: 2026-01-01
**ì‘ì„±ì**: Claude Code
